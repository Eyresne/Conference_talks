{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(prediction, Y, bprop=False):\n",
    "    if bprop:\n",
    "        return -1.0 * Y / prediction + (1.0 - Y) / (1.0 - prediction)\n",
    "    else:\n",
    "        return -1.0 * Y * np.log(prediction) - (1.0 - Y) * (np.log(1.0 - prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkActivation(\n",
    "        hidden_neurons=[50],\n",
    "        outputs=10,\n",
    "        loss_function=cross_entropy, \n",
    "        learning_rate=0.1, \n",
    "        learning_rate_layer_decay=1,\n",
    "        momentum=0.1,\n",
    "        dropout=0.8, \n",
    "        activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=True,\n",
    "                          learning_rate_anneal=1)\n",
    "accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test, predict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkActivation(\n",
    "        hidden_neurons=[50],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.1, \n",
    "        learning_rate_layer_decay=1,\n",
    "        momentum=0.1,\n",
    "        dropout=0.8, \n",
    "        activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=True,\n",
    "                          learning_rate_anneal=1)\n",
    "accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test, predict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkActivation(\n",
    "        hidden_neurons=[50],\n",
    "        outputs=10,\n",
    "        loss_function=cross_entropy, \n",
    "        learning_rate=0.1, \n",
    "        learning_rate_layer_decay=1,\n",
    "        momentum=0.1,\n",
    "        dropout=0, \n",
    "        activation_function=leaky_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=True,\n",
    "                          learning_rate_anneal=1)\n",
    "accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test, predict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We may want to decay the learning rate over time, so that the net learns faster at first and then slows its learning over time. See [here](http://cs231n.github.io/neural-networks-3/#anneal) for example. \n",
    "\n",
    "Here's how we might implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train_learning_rate_decay(net, X_train, Y_train, epochs=5, print_msg=True,\n",
    "                              learning_rate_anneal=2.0):\n",
    "    X_train, Y_train = shuffle_data(X_train, Y_train)\n",
    "    \n",
    "    learning_rate_schedule = np.linspace(net.learning_rate * learning_rate_anneal, \n",
    "                                         net.learning_rate / learning_rate_anneal, \n",
    "                                         epochs)\n",
    "    for i in range(epochs):\n",
    "        setattr(net, \"learning_rate\", learning_rate_schedule[i])\n",
    "        if print_msg:\n",
    "            print(\"Learning rate on epoch\", i+1, \n",
    "                  \"is\", net.learning_rate)\n",
    "        one_epoch(net, X_train, Y_train)\n",
    "        if print_msg:\n",
    "            print(\"Done with epoch\", i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkLR(\n",
    "        hidden_neurons=[75, 25],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=5, print_msg=True,\n",
    "                              learning_rate_anneal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if train_all:\n",
    "    accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test)\n",
    "    print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Different Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01, bprop=False):\n",
    "    if bprop:\n",
    "        dx = np.full(x.shape, alpha)\n",
    "        dx[x >= 0] = 1\n",
    "        return dx\n",
    "    else:\n",
    "        return np.maximum(alpha * x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, bprop=False):\n",
    "    if bprop:\n",
    "        e = np.exp(2*cap_sigmoid_input(x))\n",
    "        return (e-1)/(e+1)\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_sigmoid_input(a):\n",
    "    a[a < -100] = -100\n",
    "    a[a > 100] = 100\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def setup_layers(hidden_neurons, outputs, \n",
    "                 learning_rate=1.0, \n",
    "                 learning_rate_layer_decay=1.0, \n",
    "                 momentum=0.1,\n",
    "                 activation_function=sigmoid):\n",
    "    layers = []\n",
    "    for i in range(len(hidden_neurons)):\n",
    "        layer = FullyConnectedXavier(neurons=hidden_neurons[i], activation_function=activation_function)\n",
    "        setattr(layer, \"learning_rate\", learning_rate / (learning_rate_layer_decay ** i))\n",
    "        setattr(layer, \"momentum\", momentum)\n",
    "        layers.append(layer)\n",
    "\n",
    "    output_layer = FullyConnectedXavier(neurons=outputs, activation_function=sigmoid)\n",
    "    setattr(output_layer, \"learning_rate\", learning_rate / (learning_rate_layer_decay ** (len(hidden_neurons) + 1)))\n",
    "    setattr(output_layer, \"momentum\", momentum)\n",
    "    layers.append(output_layer)\n",
    "    return layers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkActivation(NeuralNetworkDropout):\n",
    "    def __init__(self, hidden_neurons, outputs, loss_function, learning_rate, \n",
    "                 learning_rate_layer_decay, momentum, dropout, activation_function):\n",
    "        NeuralNetworkDropout.__init__(self, hidden_neurons, outputs, loss_function, \n",
    "                                      learning_rate, learning_rate_layer_decay, momentum, dropout)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        \n",
    "    def forwardpass(self, X, predict=False):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        \n",
    "        if not self.layers_setup:\n",
    "            self.layers = setup_layers(self.hidden_neurons, \n",
    "                                       self.outputs, \n",
    "                                       self.learning_rate,\n",
    "                                       self.learning_rate_layer_decay, \n",
    "                                       self.momentum, \n",
    "                                       self.activation_function)\n",
    "            self.layers_setup = True\n",
    "\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if self.dropout and not predict:\n",
    "                zero_indices = np.random.choice(range(layer.n_neurons), \n",
    "                                                size=int(layer.n_neurons * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                if X_next is None:\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist_lr = NeuralNetworkActivation(\n",
    "        hidden_neurons=[50],\n",
    "        outputs=10,\n",
    "        loss_function=mean_square_error, \n",
    "        learning_rate=0.1, \n",
    "        learning_rate_layer_decay=1,\n",
    "        momentum=0.1,\n",
    "        dropout=0.8, \n",
    "        activation_function=leaky_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_learning_rate_decay(nn_mnist_lr, X_train, Y_train, epochs=1, print_msg=True,\n",
    "                          learning_rate_anneal=1)\n",
    "accuracy = net_accuracy(nn_mnist_lr, X_test, Y_test, predict=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
