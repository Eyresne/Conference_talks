{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "1. What are they?\n",
    "2. What makes them work?\n",
    "3. What is their future?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are GANs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are neural nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've all seen diagrams like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/neural_network_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what are neural nets, mathematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are:\n",
    "\n",
    "* Universal function approximators\n",
    "* Differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If each layer is written as $a$, $b$, $c$, with weights $V$ and $W$, then the prediction can be written as:\n",
    "\n",
    "$$ P = p(c(b(a(x, V)), W)) $$\n",
    "\n",
    "And the loss can be written as:\n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does differentiable mean? It means we can compute:\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial W} $$\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial V} $$\n",
    "\n",
    "etc. Indeed, this is the information we need to \"train\" the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In addition**, it also means we can compute:\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial X} $$\n",
    "\n",
    "In other words, how much the loss would change if the _input_ changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was _this_ insight that sparked Ian Goodfellow to investigate GANs:\n",
    "\n",
    "Could a machine learning algorithm use this information to learn how to \"trick\" another algorithm by producing examples that reduced this loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Origin story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ian Goodfellow and Yoshua Bengio are about to run a speech synthesis contest. They want to have a discriminator network that could listen to artificially generated speech and decide if it was real or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They conclude that people will just game the system by generating examples that will fool this particular discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, Ian Goodfellow was in a bar one night, and asked the question: can this be changed by the discriminator learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How could you do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: randomly generate a feature vector; feed the feature vector through a randomly initialized neural network to produce an output image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{bmatrix}z_1 \\\\\n",
    "                  z_2 \\\\\n",
    "                  ... \\\\\n",
    "                  z_{100}\n",
    "                  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/gan_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote the matrix of pixels in this image $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, feed this image (matrix of pixels X) into a second network and get a prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/gan_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this loss to train the generator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Critically, also compute $$ \\frac{\\partial L}{\\partial X} $$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, update the generator with $$ -\\frac{\\partial L}{\\partial X} $$\n",
    "\n",
    "negative because we want the generator to be continually making the discriminator more likely to say that its images are real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to update the weights of the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/gan_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate _new_ random noise, and repeat the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's missing?\n",
    "\n",
    "This will train the generator to generate good fake images, but it will likely result in the discriminator not being a very smart classifier since we only gave it one of the two classes it is trying to classify. So, we'll have to give it images from the true class as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/gans_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Original GitHub repo with Ian Goodfellow's code](https://github.com/goodfeli/galatea/commit/d960968919b0856ba6753198a0e035228d7c03e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code one up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See notebook here [here](GAN_example/dlnd_face_generation.ipynb). TODO: get this running on AWS (easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've all seen diagrams like this in the context of convolutional neural nets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/AlexNet_0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's really going on here?\n",
    "\n",
    "Let's say we have an input layer of size $[224x224x3]$, as we do in ImageNet. This next layer seems to be $96$ deep. What does that mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of 96 _filters_, the following happens:\n",
    "\n",
    "For each of the 3 _input channels_, the _filter_, which happens to be dimension $11 x 11$ in this case, is slid over the image, \"detecting the presence of different features\" at each location. We'll call the image that results the \"output image\".\n",
    "\n",
    "At the next layer, the values of these three output images are summed together to generate the first of 96 output images in the following layer. \n",
    "\n",
    "In addition, the three filters - the one slid over the red, green, and blue color channels - can be combined together and visualized as if they were a mini 11x11 image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/AlexNet_filt1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a review of convolutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point is: for each convolution, the **size of the output** from convolving a filter over an image will be a function of:\n",
    "\n",
    "1. The input size, $I$\n",
    "2. The filter size, $F$\n",
    "3. The stride, $S$\n",
    "4. The padding, $P$\n",
    "\n",
    "There are formulas relating these quantities that you can look up; however, I think it's best to just reason through what the output shape should be each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this have to do with GANs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing these convolutions is relatively straightforward - but how do we do deconvolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning, if we start with a $4x4$ input, how do we scale it up to say, $28x28$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we can represent convolutions by a matrix transformation. See [here](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html#convolution-as-a-matrix-operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all, a convolution that transforms a $4x4$ image into a $2x2$ is a linear transformation that can be represented by a $16x4$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To \"deconvolve\" a $2x2$ matrix into a $4x4$, we would multiply it by the inverse of the original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, out_channel_dim, is_train=True):\n",
    "    \"\"\"\n",
    "    Create the generator network\n",
    "    :param z: Input z\n",
    "    :param out_channel_dim: The number of channels in the output image\n",
    "    :param is_train: Boolean if generator is being used for training\n",
    "    :return: The tensor output of the generator\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    with tf.variable_scope('generator', reuse=not is_train):\n",
    "        # First fully connected layer\n",
    "        x1 = tf.layers.dense(z, 4*4*512)\n",
    "#         # Reshape it to start the convolutional stack\n",
    "        x1 = tf.reshape(x1, (-1, 4, 4, 512))\n",
    "        x1 = tf.layers.batch_normalization(x1, training=is_train)\n",
    "        x1 = tf.maximum(0.2 * x1, x1)\n",
    "\n",
    "        x2 = tf.layers.conv2d_transpose(x1, 256, 4, strides=1, padding='same')\n",
    "        x2 = tf.layers.batch_normalization(x2, training=is_train)\n",
    "        x2 = tf.maximum(0.2 * x2, x2)\n",
    "\n",
    "#         x3 = tf.layers.conv2d_transpose(x2, 128, 5, strides=2, padding='same')\n",
    "#         x3 = tf.layers.batch_normalization(x3, training=is_train)\n",
    "#         x3 = tf.maximum(0.2 * x3, x3)\n",
    "\n",
    "#         logits = tf.layers.conv2d_transpose(x3, out_channel_dim, 5, strides=2, padding='same')\n",
    "\n",
    "#         out = tf.tanh(logits)\n",
    "    \n",
    "    return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import mock\n",
    "class TmpMock():\n",
    "    \"\"\"\n",
    "    Mock a attribute.  Restore attribute when exiting scope.\n",
    "    \"\"\"\n",
    "    def __init__(self, module, attrib_name):\n",
    "        self.original_attrib = deepcopy(getattr(module, attrib_name))\n",
    "        setattr(module, attrib_name, mock.MagicMock())\n",
    "        self.module = module\n",
    "        self.attrib_name = attrib_name\n",
    "\n",
    "    def __enter__(self):\n",
    "        return getattr(self.module, self.attrib_name)\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        setattr(self.module, self.attrib_name, self.original_attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 4, 256)\n"
     ]
    }
   ],
   "source": [
    "with TmpMock(tf, 'variable_scope') as mock_variable_scope:\n",
    "    z = tf.placeholder(tf.float32, [None, 100])\n",
    "    out_channel_dim = 3\n",
    "    output = generator(z, out_channel_dim)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's review two common terms when it comes to \"padding\" in TensorFlow or convolutional neural nets more generally:\n",
    "\n",
    "* \"Same\" padding means pad the input so that the output shape is the same as the input shape. If we have a 5x5 kernel, that means pad two units on the edges of the image.\n",
    "* \"Valid\" padding means don't apply any padding - on a forward convolution pass, having no padding means the output image will be smaller than the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why is it that applying a `conv_2d_transpose` operation to a $4x4$ image with `valid` padding results in a $7x7$ whereas applying that operation with `same` padding results in a $4x4$? On the forward pass, this is easy to reason about, but on the backwards pass it is trickier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is that deconvolutions are just inverses of convolutions - and applying a convolution of kernel size 4 and stride 1 with `valid` padding to a 7x7 image would result in a 4x4. Similar reasoning applys to same padding. TODO: add illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the dimensions right on these deconvolutions is one of the trickiest parts of getting GANs to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we do when we train neural nets? At each iteration, we update the weights based on $ \\frac{\\partial l}{\\partial W} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this really accurate? Well, it's a first order approximation - literally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can indeed figure out how much the weights should change _hodling everything else in the neural net constant_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, all the weights in the neural net are changing at once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can lead to significant \"second order effects\", where changing one weight can have a much different impact on the loss than we expect because its change interacts with all the other weights' changes as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get concrete, imagine for a given weight $ w $, and we compute$ \\frac{\\partial l}{\\partial w} = 0.1 $: increasing this weight by 1 unit will increase the loss by 1 unit. With the interactions with all the other weight changes, the actual increase could be significantly different than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch normalization** significantly mitigates this problem. It involves the following:\n",
    "\n",
    "1. For each batch of images (typically 64 or 128) fed through the network:\n",
    "At each layer, calculate the mean $\\mu$ and the standard deviation $\\sigma$ for all the neurons in the layer.\n",
    "2. Normalize each neuron by subtracting off the mean and standard deviation:\n",
    "\n",
    "$$N' = \\frac{N - \\mu}{\\sigma}$$\n",
    "\n",
    "Continue propagating through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can anyone think of an issue with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convolutional networks, the \"neurons\" are pixels in output images that have been convolved with a filter. These images are important - they contain spatial information about what is present in the images. If we modify pixels in these images by different amounts, this spatial information could get modified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have to calculate $F$ means and standard deviations for each batch and each filter map, so that in a given filter map, each pixel will be \"normalized\" by the same amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There's more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't stop there. We further modify $N'$ to be defined as:\n",
    "\n",
    "$$ \\gamma * N' + \\beta $$\n",
    "\n",
    "We initialize $\\gamma$ to 1 and $\\beta$ to 0. And then these become parameters that are learned along with all the others in the course of the network training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: why does this work? Why do we normalize _and then_ apply these parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that the mean of a given layer of features is significant to determining the behavior of the following layer - you can either think of the mean of a hidden layer of neurons, or the mean value across a filter in a convolutional layer. Without normalizing and then applying $\\gamma$ and $\\beta$, the network will have to learn the mean of this layer by adjusting individual weights. \n",
    "\n",
    "By applying these transformations, however, the network can simply learn on parameter $\\beta$ that determines the mean of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 8.7 of [http://www.deeplearningbook.org](the definitive textbook on Deep Learning) explains this well:\n",
    "\n",
    "> ...the new parametrization can represent the same family of functions of the input as the old parametrization, but the new parametrization has diﬀerent learning dynamics. In the old parametrization, the mean of [the layer] was determined by a complicated interaction between the parameters in the layers below. In the new parametrization, the mean of is determined solely by $\\beta$. The new parametrization is much easier to learn with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC-GAN Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three tricks:\n",
    "\n",
    "## Deconvolutions:\n",
    "\n",
    "Architecture:\n",
    "\n",
    "1. Start with random noise vector of length 100\n",
    "2. Begin by transforming it to a fully connected layer with shape $4*4*512$, say.\n",
    "3. Perform deconvolution steps to apply a bunch of filters to these images to produce images of the shape you want, say 28 x 28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through an illustration here:\n",
    "\n",
    "[Theano documentation](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)\n",
    "\n",
    "What do deconvolution operations do? \n",
    "Convolutions can be represented as matrix multiplications. \n",
    "Deconvolutions are just the inverse of those matrix multiplications.\n",
    "So, when choosing our filter, padding, and stride of an inverse convolution operation, what output would result?\n",
    "The answer is that the resulting output will be the one that would produce the input that we give this layer.\n",
    "\n",
    "Let's look at an example. If we give a Conv 2D transpose a 4x4, and do an inverse convolution operation with:\n",
    "\n",
    "Stride 1\n",
    "Filter size 4\n",
    "Valid padding.\n",
    "\n",
    "We get a 7x7. Why?\n",
    "\n",
    "Consider doing this operation on a 7x7. It is clear that we'll get a 4x4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](http://www.deeplearningbook.org/contents/optimization.html), and especially the batch normalization lesson in the Udacity repo. This guy actually figured out the gradients for batch normalization. [here](http://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html). Also: [unbelievable](https://github.com/cthorey/CS231)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is their future?\n",
    "\n",
    "## Applications:\n",
    "\n",
    "* Drug discovery\n",
    "\n",
    "* Semi-supervised learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-supervised learning is a third type of machine learning, other than supervised learning and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSL: \n",
    "\n",
    "1. Discriminator computes both probabilities of class belonging to each of 10 classes, probability of it being real. Note: it computes the probability of the images being real on both real and fake data, so that you \n",
    "\n",
    "How do we make the generator smarter? \"Feature matching\".\n",
    "\n",
    "1. The last layer of a convolutinoal netural network, before it gets reshaped to a fully connected layer, is usually a long, narrow layer. In the network we just looked at, the last layer was 2x2x128. We can take the average over these 128 filters to get a dimension 128 vector. This vector \"should\" represent the features that the network has extracted from the image that ultimately determine what class the image belongs to, whether it is real or fake etc.\n",
    "2. We can compute this vector for both real or generated images fed through the discrminator. \n",
    "3. We can compare the values of these features for the real images vs. the generated images to get a loss.\n",
    "\n",
    "This is called **\"feature matching\"** - the main idea is to consider the fetaures that the discriminator while \"discrminating\" between the real and fake images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the trick that led to breakthrough performance using semi-supervised learning to build powerful classifiers: \n",
    "\n",
    "Salimans et. al. from OpenAI in mid-2016 used this approach to get 6% error rate on the Street View House Numbers dataset _with just 1,000 labeled images_ - state-of-the-art, using the entire dataset of roughly 600,000 images, is 2% simply using supervised learning with very deep convolutional networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since then, hybrid approaches that used both feature matching _and_ the traditional GAN loss have been shown to work - see [here](http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/) for example. \n",
    "\n",
    "More recently, researchers from CMU have shown that a semi-supervised learning approach that prevents the GAN from \"getting too good\" by penalizing it for generating images too similar to those in the training set - using a \"bad GAN\" as they call it - outperforms the feature matching approach. \n",
    "\n",
    "Again, the central idea - that the GAN shouldn't actually simply be trained to generate the best possible images - is more important than the specific details of the penalty they used. (see the paper [here](https://www.cs.cmu.edu/~wcohen/postscript/nips-2017-badgan.pdf) - especially the second-to-last paragraph of the introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive GANs\n",
    "\n",
    "People have been trying to improve the resolution of GANs since their inception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](http://research.nvidia.com/sites/default/files/publications/karras2017gan-paper.pdf) is the Progressive GAN paper. This technique was able to generate high quality 32x32 images mimicking those from the CelebA dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did they do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental idea was:\n",
    "\n",
    "1. Begin by downsampling the images to be simply _4x4_.\n",
    "2. Train a GAN to generate \"high quality\" 4x4 images. \n",
    "3. Then, using the weights already learned in the initial layers, add a layer after the generator and before the discriminator so that this GAN now generates _8x8_ images (see illustration).\n",
    "4. Even better, when these larger layers are initially added, there's a \"grace period\" where the generated images are still _mostly_ a function of the weights of the layers that have already been trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"When new layers are added to the networks, we fade them in smoothly...This avoids sudden shocks to the already well-trained, smaller-resolution layers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows them to train the network using something known as \"Wasserstein loss\", a more sophisticated measure of how similar two distributions are. (See the Introduction to the Wasserstein GAN paper [here](https://arxiv.org/pdf/1701.07875.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several other clever tricks they use. See the paper, especially Sections 2, 3, and 4 (a total of two pages) for details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know how \"good\" GANs are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out if you score GANs naively (e.g. using mean squared error), they don't turn out to be much \"better\" than other popular image generation methods, such as Variational Autonencoders.\n",
    "\n",
    "As Ian Goodfellow himself noted in his 2016 NIPS tutorial on GANs, there isn't an obvious way to score GANs, and one of their advantages is that they produce images that simply \"look\" better, even if it is hard to quantify exactly what this means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clever method for scoring GANs was developed by Tim Salimans at OpenAI, that illustrates well some properties that we want GANs to have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, observe that if a GAN generated some images, and those images were then fed through a pre-trained neural network, and the resulting probability distribution over the images was:\n",
    "\n",
    "`[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]`\n",
    "\n",
    "this probably isn't a very good GAN.\n",
    "\n",
    "The way we formalize this is that this resulting vector should have _low entropy_ - that is, _not_ an even distribution over class labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, let's say that every time, the \"most likely class\" that the pre-trained network was predicting was a zero. We don't want this: we want to generator to generate diverse images. \n",
    "\n",
    "The way we formalize this is that we want the vector of the frequency of the predictions to have _high entropy_: that is, class balance. So, if our generator generates 1000 MNIST images, we want 100 of them to be predicted to be 0s, 100 to be predicted to be 1s etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception score - named after the architecture used to make predictions with the generated images - is actually used to score the different models in the [Progressive GAN paper](http://research.nvidia.com/sites/default/files/publications/karras2017gan-paper.pdf) (see page 15)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wasserstein distance is a pixel-based measure proposed in the Progressive GAN paper that represents a totally different way of measuring of how similar the generated images are to the real images. \n",
    "\n",
    "The innovation, however, is that the similarity measure used is not a normal similiarity measure, but instead is based on examining the similarity between the 16x16 versions of the images, the 32x32 version, and so on up to the final 1024x1024 version. More specifically, many random 7x7 patches from all these different images (both the generated and real images) are sampled, and the similarity between all these 7x7 patches is calculated. As the authors of the paper put it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Intuitively a small Wasserstein distance indicates that the distribution of the patches is similar, meaning that the training images and generator samples appear similar in both appearance and variation at this spatial resolution. In particular, the distance between the patch sets extracted from the lowest-resolution 16 × 16 images indicate similarity in large-scale image structures, while the finest-level patches encode information about pixel-level attributes such as sharpness of edges and noise.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
